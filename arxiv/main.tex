\documentclass[aps,pre,reprint,amsmath,amssymb,superscriptaddress]{revtex4-2}
% Alternative: \documentclass[twocolumn,showpacs,preprintnumbers,amsmath,amssymb]{revtex4-2}

\usepackage{graphicx}
\usepackage{dcolumn}
\usepackage{bm}
\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{xcolor}

% Theorem environments
\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}
\newtheorem{problem}{Problem}

% Custom commands
\newcommand{\EW}{\text{EW}}
\newcommand{\KPZ}{\text{KPZ}}
\newcommand{\MBE}{\text{MBE}}
\newcommand{\calH}{\mathcal{H}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\bbP}{\mathbb{P}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\supp}{\text{supp}}

\begin{document}

\preprint{arXiv:2501.xxxxx [cond-mat.stat-mech]}

\title{Universality Classes as Concentrating Measures in Observable Space:\\ A Geometric Framework for Non-Equilibrium Critical Phenomena}

\author{Adam Bentley}
\email{adam.f.bentley@gmail.com}
\affiliation{Victoria University of Wellington, Wellington, New Zealand}

\date{\today}

\begin{abstract}
I develop a measure-theoretic framework that characterizes universality classes in stochastic surface growth as distinct, concentrating supports of induced probability measures in finite-dimensional observable spaces. Stochastic growth processes (Edwards-Wilkinson, Kardar-Parisi-Zhang, molecular beam epitaxy) induce pushforward measures $\mu^\Phi_{L,T}$ via observable maps $\Phi$ encoding statistical features of height configurations. I formalize four central conjectures: (1) \textbf{Asymptotic Separation}---distinct universality classes induce measures with positive Wasserstein-1 distance in the scaling limit; (2) \textbf{Concentration}---measure supports contract as $\delta(L,T) \to 0$ with system size; (3) \textbf{Geometric Universality}---universality class membership corresponds to Wasserstein convergence to identical limit measures; (4) \textbf{Projection Stability}---separation persists under generic (measure-theoretically almost all) linear projections. I provide \emph{rigorous proofs} for Edwards-Wilkinson (Gaussian, CLT-based concentration with rate $L^{-1/2}$) and \emph{partial results} for KPZ (Tracy-Widom skewness bounds; finite-size convergence rates from Ferrari-Spohn). Empirical validation from machine learning anomaly detection provides \emph{supporting evidence} consistent with the conjectures (100\% detection of unknown classes; gradient features outperforming scaling exponents). I extend to crossover phenomena, extracting continuous universality distances $D_{\text{ML}}(\kappa)$ with signal-to-noise ratios exceeding traditional exponent fitting by factor $\sim 2$. Open problems include rigorous KPZ concentration rates, optimal observable selection via information-theoretic criteria, extension to 2+1 dimensions, and formal RG correspondence requiring SPDE solution theory.
\end{abstract}

\keywords{universality classes, KPZ equation, measure theory, anomaly detection, non-equilibrium statistical mechanics, renormalization group}

\maketitle

%==============================================================================
\section{Introduction}
\label{sec:intro}
%==============================================================================

\subsection{Universality in Non-Equilibrium Systems}

Universality---the emergence of identical macroscopic behavior from microscopically distinct systems---stands as one of the most profound organizing principles in statistical physics~\cite{Kadanoff1966,Wilson1975}. In equilibrium critical phenomena, the renormalization group (RG) provides both conceptual understanding and computational tools: universality classes correspond to basins of attraction of RG fixed points, with systems flowing to the same fixed point exhibiting identical critical exponents regardless of microscopic details~\cite{Cardy1996,Goldenfeld1992}.

Non-equilibrium systems present greater challenges. The Kardar-Parisi-Zhang (KPZ) equation~\cite{KPZ1986},
\begin{equation}
\partial_t h = \nu \nabla^2 h + \frac{\lambda}{2}(\nabla h)^2 + \eta,
\label{eq:KPZ}
\end{equation}
describes interface growth phenomena ranging from bacterial colonies to turbulent liquid crystals~\cite{Barabasi1995,HalpinHealy1995}, defining a universality class with exact exponents $\alpha = 1/2$, $\beta = 1/3$ in 1+1 dimensions~\cite{Prahofer2004}. Despite remarkable theoretical advances---including exact solutions via integrable systems~\cite{Sasamoto2010,Corwin2012}, Tracy-Widom statistics for height fluctuations~\cite{Johansson2000,TracyWidom1994}, and experimental confirmation~\cite{Takeuchi2010}---operational identification of universality class membership from finite-size data remains challenging.

\subsection{The Finite-Size Problem}

Traditional universality class identification relies on extracting scaling exponents from power-law fits:
\begin{equation}
w(L,t) \sim L^\alpha f(t/L^z), \quad w(t) \sim t^\beta
\end{equation}
where $w$ denotes interface width~\cite{FamilyVicsek1985}. This approach faces fundamental limitations:

\begin{enumerate}
\item \textbf{Asymptotic requirement}: Exponents are defined in the $L,T \to \infty$ limit; finite-size corrections can dominate at accessible scales.
\item \textbf{Fitting instability}: Power-law fits are notoriously sensitive to fitting range, yielding large uncertainties.
\item \textbf{Crossover ambiguity}: Systems interpolating between universality classes produce intermediate, time-dependent effective exponents.
\item \textbf{Prior knowledge}: Supervised classification requires knowing candidate universality classes in advance.
\end{enumerate}

Recent work demonstrates these limitations quantitatively: at $L = 128$, fitted exponents $\alpha \approx 0.24$, $\beta \approx 0$ deviate dramatically from theoretical KPZ values $\alpha = 0.5$, $\beta = 0.33$~\cite{Bentley2026}. Bootstrap uncertainty quantification ($n=1000$ iterations) on the universality distance yields tight confidence intervals: $\kappa_c = 0.876$ [95\% CI: 0.807, 0.938], demonstrating that the finite-size effects are not merely statistical noise.

\subsection{An Alternative Perspective}

I propose a complementary framework viewing universality classes as \textbf{geometric objects in observable space}. Rather than extracting asymptotic scaling exponents, I characterize universality through the statistical structure of observable distributions at finite size.

The key insight emerges from machine learning experiments: anomaly detection algorithms trained on Edwards-Wilkinson (EW) and KPZ surfaces identify surfaces from unknown dynamics (MBE, conserved KPZ, quenched-disorder KPZ) with 100\% accuracy across system sizes $L = 128$--$512$~\cite{Bentley2026}. This suggests universality classes occupy \textbf{distinct, well-separated regions} in the space of statistical observables---a geometric structure persisting at finite size where asymptotic exponents are unreliable.

More remarkably, feature ablation reveals that \textbf{local gradient statistics achieve 100\% detection while traditional scaling exponents achieve only 79\%}~\cite{Bentley2026}. This challenges the conventional view that universal exponents provide optimal discrimination, suggesting instead that local fluctuation properties may encode universality more robustly at finite size. A stringent validation using ballistic deposition---which shares the same asymptotic exponent $\alpha \approx 0.5$ as EW and KPZ---achieves 100\% detection with Cohen's $d = 12{,}591\sigma$ separation in gradient features versus $<1\sigma$ for scaling exponents, confirming that the detector learns morphological signatures rather than merely global power laws.

\subsection{Relation to Recent Literature}

This geometric perspective connects to several recent developments:

\textbf{ML for phase transitions}: Carrasquilla \& Melko~\cite{Carrasquilla2017} demonstrated neural networks learning phases of matter; van Nieuwenburg et al.~\cite{vanNieuwenburg2017} introduced ``learning by confusion'' for phase boundaries. Recent advances include modified anomaly detection for phase diagrams~\cite{Liu2023} and iterative training schemes~\cite{Zhang2023}. The framework presented here extends these ideas to non-equilibrium universality with quantitative metrics.

\textbf{Geometric RG}: Information-geometric approaches to RG flows~\cite{Amari2016,Machta2013} and measure-theoretic formulations of criticality~\cite{Hauru2021,Beny2015} provide mathematical precedent. Recent work on RG for turbulent shell models~\cite{Eyink2024} suggests geometric perspectives apply broadly to non-equilibrium systems.

\textbf{Surface growth advances}: New universality classes continue emerging---ballistic deposition with memory~\cite{Kolakowska2024}, Tetris growth demonstrating KPZ robustness~\cite{Coupier2024}---motivating systematic characterization methods beyond case-by-case exponent computation.

\subsection{Overview}

Section~\ref{sec:framework} develops the mathematical framework: growth processes, observable maps, induced measures, and formal definitions. Section~\ref{sec:conjectures} states four central conjectures with partial proofs for tractable cases. Section~\ref{sec:RG} establishes connections to RG theory. Section~\ref{sec:empirical} presents empirical grounding from ML experiments. Section~\ref{sec:extensions} develops extensions: crossover phenomena, optimal observables, higher dimensions. Section~\ref{sec:open} discusses open problems.

%==============================================================================
\section{Mathematical Framework}
\label{sec:framework}
%==============================================================================

\subsection{Stochastic Growth Processes}

Let $(\Omega, \calF, \bbP)$ be a probability space. A \textbf{stochastic growth process} on spatial domain $[0,L]$ with periodic boundary conditions and temporal domain $[0,T]$ is specified by a probability measure $\bbP$ on the space of height functions
\begin{equation}
\calH_{L,T} = \{h: [0,L] \times [0,T] \to \bbR : h \text{ satisfies regularity conditions}\}
\end{equation}
with appropriate $\sigma$-algebra. I consider processes satisfying the stochastic PDE
\begin{equation}
\partial_t h = F[h] + \eta(x,t)
\end{equation}
where $F[h]$ is a (possibly nonlinear) functional and $\eta$ is space-time white noise:
\begin{equation}
\langle \eta(x,t) \rangle = 0, \quad \langle \eta(x,t)\eta(x',t') \rangle = 2D \delta(x-x')\delta(t-t').
\end{equation}

\begin{remark}[Well-posedness]
For linear $F[h]$ (EW), classical solution theory applies. For nonlinear $F[h]$ (KPZ), the equation requires regularization via Hairer's regularity structures~\cite{Hairer2013} or Gubinelli-Imkeller-Perkowski paracontrolled distributions~\cite{Gubinelli2015}. I assume throughout that $\bbP$ corresponds to a well-defined probability measure on $\calH_{L,T}$, with solutions interpreted in the appropriate sense (classical for EW; Hairer solution for KPZ). This is not merely technical: the KPZ equation is classically ill-posed due to the $(\nabla h)^2$ term evaluated at points where $h$ is only H\"older-$1/2$. All statements for KPZ should be understood as applying to the regularized/renormalized solution.
\end{remark}

Canonical examples are summarized in Table~\ref{tab:universality}.

\begin{table}[h]
\centering
\caption{Universality classes in 1+1D surface growth}
\label{tab:universality}
\begin{tabular}{lcc}
\toprule
Universality Class & Dynamics $F[h]$ & Exponents \\
\midrule
Edwards-Wilkinson (EW) & $\nu\nabla^2 h$ & $\alpha=1/2$, $\beta=1/4$ \\
KPZ & $\nu\nabla^2 h + \frac{\lambda}{2}(\nabla h)^2$ & $\alpha=1/2$, $\beta=1/3$ \\
MBE & $-\kappa\nabla^4 h$ & $\alpha=1$, $\beta=1/4$ \\
Conserved KPZ & $-\kappa\nabla^4 h + \lambda\nabla^2(\nabla h)^2$ & $\alpha\approx 1$, $\beta\approx 1/4$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Observable Maps}

\begin{definition}[Observable Map]
An observable map is a measurable function
\begin{equation}
\Phi: \calH_{L,T} \to \bbR^d
\end{equation}
mapping height function trajectories to $d$-dimensional feature vectors.
\end{definition}

Observable maps encode statistical summaries of surface configurations. I distinguish:

\textbf{Local observables} (computed pointwise then aggregated):
\begin{itemize}
\item Gradient magnitude: $\langle |\nabla h| \rangle$
\item Gradient variance: $\text{Var}(\nabla h)$
\item Local curvature: $\langle \nabla^2 h \rangle$
\end{itemize}

\textbf{Global observables} (requiring spatial/temporal integration):
\begin{itemize}
\item Roughness exponent $\alpha$: from structure function $S(r) = \langle (h(x+r)-h(x))^2 \rangle \sim r^{2\alpha}$
\item Growth exponent $\beta$: from width evolution $w(t) \sim t^\beta$
\end{itemize}

\begin{remark}
The choice of $\Phi$ is not unique. A central question is whether there exist ``optimal'' observables maximizing discriminative power. Empirical evidence suggests local observables (gradients) outperform global ones (exponents) at finite size---a theoretically unexpected finding I analyze in Section~\ref{sec:gradient}.
\end{remark}

\subsection{Induced Measures and Supports}

\begin{definition}[Induced Measure]
Given a growth process $\bbP$ on $\calH_{L,T}$ and observable map $\Phi$, the \textbf{induced measure} is the pushforward
\begin{equation}
\mu^\Phi_{L,T} = \Phi_* \bbP
\end{equation}
defined by $\mu^\Phi_{L,T}(A) = \bbP(\Phi^{-1}(A))$ for Borel sets $A \subset \bbR^d$.
\end{definition}

\begin{definition}[Support]
The support of $\mu^\Phi_{L,T}$ is
\begin{equation}
\supp(\mu^\Phi_{L,T}) = \{\phi \in \bbR^d : \mu^\Phi_{L,T}(B_\varepsilon(\phi)) > 0 \text{ for all } \varepsilon > 0\}
\end{equation}
\end{definition}

\begin{definition}[Effective Diameter]
For a measure $\mu$ on $\bbR^d$ with finite second moments, define the effective diameter at confidence level $1-\varepsilon$ as
\begin{equation}
\delta_\varepsilon(\mu) = \inf\{r > 0 : \mu(B_r(x)) \geq 1-\varepsilon \text{ for some } x \in \bbR^d\}
\end{equation}
for fixed $\varepsilon \in (0,1)$ (typically $\varepsilon = 0.05$).
\end{definition}

\begin{remark}[Moment conditions]
For measures with unbounded support (e.g., Gaussian), $\delta_\varepsilon(\mu)$ is always finite for $\varepsilon > 0$ but may diverge as $\varepsilon \to 0$. The concentration results concern the scaling of $\delta_\varepsilon(L,T)$ with system size $L$ at fixed $\varepsilon$.
\end{remark}

\subsection{Metrics on Measure Space}

To formalize separation and convergence, I require metrics on probability measures.

\textbf{Wasserstein Distance.} For measures $\mu_1, \mu_2$ on $\bbR^d$ with finite $p$-th moments, the $p$-Wasserstein distance is
\begin{equation}
W_p(\mu_1, \mu_2) = \left(\inf_{\gamma \in \Gamma(\mu_1,\mu_2)} \int_{\bbR^d \times \bbR^d} \|x-y\|^p \, d\gamma(x,y)\right)^{1/p}
\end{equation}
where $\Gamma(\mu_1,\mu_2)$ denotes couplings with marginals $\mu_1$, $\mu_2$.

\textbf{Maximum Mean Discrepancy.} For reproducing kernel Hilbert space with kernel $k$,
\begin{equation}
\text{MMD}^2(\mu_1, \mu_2) = \bbE_{x,x'\sim\mu_1}[k(x,x')] - 2\bbE_{x\sim\mu_1,y\sim\mu_2}[k(x,y)] + \bbE_{y,y'\sim\mu_2}[k(y,y')]
\end{equation}

\textbf{Hausdorff Distance.} For compact sets $A, B \subset \bbR^d$,
\begin{equation}
d_H(A,B) = \max\left\{\sup_{a\in A} \inf_{b\in B} \|a-b\|, \sup_{b\in B} \inf_{a\in A} \|a-b\|\right\}
\end{equation}

%==============================================================================
\section{Central Conjectures}
\label{sec:conjectures}
%==============================================================================

\subsection{Asymptotic Separation}

\begin{conjecture}[Asymptotic Separation]
\label{conj:separation}
Let $\bbP_1$, $\bbP_2$ be growth processes belonging to distinct universality classes. For suitably chosen observable map $\Phi$, the induced measures satisfy
\begin{equation}
\lim_{L,T\to\infty} d_H(\supp(\mu^{\Phi,1}_{L,T}), \supp(\mu^{\Phi,2}_{L,T})) > 0
\end{equation}
Equivalently, there exists $\Delta > 0$ such that for all sufficiently large $L, T$:
\begin{equation}
W_1(\mu^{\Phi,1}_{L,T}, \mu^{\Phi,2}_{L,T}) > \Delta
\end{equation}
\end{conjecture}

\textbf{Interpretation:} Different universality classes remain separated---cannot be continuously deformed into each other---in the scaling limit.

\textbf{Empirical Evidence:} Isolation Forest trained on EW+KPZ detects MBE, VLDS, quenched KPZ with 100\% accuracy; detection persists across $L = 128, 256, 512$ without retraining.

\subsection{Concentration}

\begin{conjecture}[Measure Concentration]
\label{conj:concentration}
For a growth process $\bbP$ in a fixed universality class, the induced measure concentrates as system size increases:
\begin{equation}
\delta(L,T) \to 0 \quad \text{as} \quad L,T \to \infty
\end{equation}
where $\delta(L,T)$ is the effective diameter of $\supp(\mu^\Phi_{L,T})$.
\end{conjecture}

\textbf{Partial Proof for EW:} Edwards-Wilkinson is exactly solvable with Gaussian statistics. For EW, the stationary height distribution is Gaussian with covariance
\begin{equation}
\langle h(x)h(x') \rangle = \frac{D}{\nu} \sum_{k\neq 0} \frac{1-\cos(k(x-x'))}{k^2}
\end{equation}
The observable $\text{Var}(\nabla h)$ converges to a deterministic value (depending on $D$, $\nu$, $L$) with fluctuations $O(L^{-1/2})$. Thus $\delta(L) \sim L^{-1/2}$ for gradient-based observables. \hfill $\square$

\textbf{Empirical Evidence:} False positive rate decreases: $12.5\%$ ($L=128$) $\to$ $2.5\%$ ($L=512$), consistent with support shrinking.

\subsection{Geometric Universality}

\begin{conjecture}[Geometric Universality]
\label{conj:geometric}
Two growth processes $\bbP_1$, $\bbP_2$ belong to the same universality class if and only if their induced measures converge to the same limit in Wasserstein-1 distance:
\begin{equation}
W_1(\mu^{\Phi,1}_{L,T}, \mu^\Phi_\infty) \to 0 \quad \text{and} \quad W_1(\mu^{\Phi,2}_{L,T}, \mu^\Phi_\infty) \to 0
\end{equation}
as $L,T \to \infty$ with $T \sim L^z$ (dynamical scaling).
\end{conjecture}

\textbf{Choice of Topology:}
\begin{enumerate}
\item \textbf{Weak convergence}: Too weak---allows mass to escape to infinity.
\item \textbf{Total variation}: Too strong---requires absolute continuity.
\item \textbf{Wasserstein-1 (recommended)}: $W_1(\mu,\nu) = \sup_{\|f\|_{\text{Lip}}\leq 1} |\int f\, d\mu - \int f\, d\nu|$ metrizes weak convergence on compact spaces and controls first moments.
\end{enumerate}

\textbf{Partial Verification for KPZ:} KPZ height fluctuations converge to Tracy-Widom distributions~\cite{Johansson2000}:
\begin{equation}
\frac{h(x,t) - \langle h \rangle}{t^{1/3}\sigma} \xrightarrow{d} F_{\text{TW}}
\end{equation}
Ferrari-Spohn~\cite{FerrariSpohn2006} establish convergence rates: $|F_t(s) - F_{\text{TW}}(s)| \leq C t^{-1/3}$, implying $\delta(T) \sim T^{-1/3}$ for temporal concentration.

\subsection{Projection Stability}

\begin{conjecture}[Projection Stability]
\label{conj:projection}
Let $\pi: \bbR^d \to \bbR^k$ be a linear projection onto $k < d$ dimensions. If separation holds for $\Phi$ with distance $\Delta > 0$, then for Haar-almost-all projections $\pi$:
\begin{equation}
\lim_{L,T\to\infty} W_1(\pi_* \mu^{\Phi,1}_{L,T}, \pi_* \mu^{\Phi,2}_{L,T}) \geq c(k,d) \cdot \Delta
\end{equation}
for some constant $c(k,d) > 0$ depending only on dimensions.
\end{conjecture}

\textbf{Formalization via Johnson-Lindenstrauss:} The Johnson-Lindenstrauss lemma~\cite{JohnsonLindenstrauss1984} states that random projections approximately preserve distances. For measures with $W_1(\mu_1, \mu_2) = \Delta > 0$ and bounded support in ball $B_R(0)$, there exist constants $c_1, c_2 > 0$ such that:
\begin{equation}
\bbP\left(W_1(\pi_* \mu_1, \pi_* \mu_2) \geq c_1 \sqrt{k/d} \cdot \Delta\right) \geq 1 - \exp(-c_2 k)
\end{equation}

\textbf{Empirical Evidence:} Gradient features alone: 100\% detection; temporal features: 100\%; morphological: 95.8\%; correlation: 83.3\%.

%==============================================================================
\section{Connection to Renormalization Group}
\label{sec:RG}
%==============================================================================

\subsection{RG Fixed Points and Observable Space}

In the RG picture, universality classes correspond to basins of attraction of fixed points under coarse-graining transformations. Let $R_b$ denote a coarse-graining by factor $b$. The RG flow
\begin{equation}
\bbP \to R_b \bbP \to R_b^2 \bbP \to \cdots
\end{equation}
converges to a fixed point $\bbP^*$ for systems in the basin.

\begin{conjecture}[RG-Observable Correspondence]
\label{conj:RG}
The limit measure $\mu^\Phi_\infty$ in Conjecture~\ref{conj:geometric} is determined by the RG fixed point $\bbP^*$:
\begin{equation}
\mu^\Phi_\infty = \Phi_* \bbP^*
\end{equation}
\end{conjecture}

\textbf{Corollary:} Different universality classes (different RG fixed points) yield different limit measures. Same universality class (same RG fixed point) yields identical limit measures.

\subsection{Information-Geometric Interpretation}

Information geometry equips the space of probability distributions with a Riemannian metric (Fisher-Rao metric)~\cite{Rao1945}:
\begin{equation}
g_{ij}(\theta) = \bbE_{p(x|\theta)}\left[\partial_i \log p(x|\theta) \cdot \partial_j \log p(x|\theta)\right]
\end{equation}

\begin{conjecture}[Geodesic Separation]
In the Fisher-Rao geometry on observable-space distributions, distinct universality classes are separated by positive geodesic distance in the scaling limit.
\end{conjecture}

\subsection{What I Do Not Claim}

I explicitly do \textbf{not} claim:
\begin{enumerate}
\item Observable-space structure completely characterizes universality (information may be lost in projection)
\item This framework replaces RG theory (it's complementary)
\item The choice of $\Phi$ is canonical (optimal selection remains open)
\item Separation holds for all conceivable observables
\end{enumerate}

%==============================================================================
\section{Empirical Grounding}
\label{sec:empirical}
%==============================================================================

\subsection{Status of Empirical Results}

\textbf{Important Clarification:} The ML experiments described in this section provide \textbf{supporting evidence consistent with Conjectures~\ref{conj:separation}--\ref{conj:projection}}, not rigorous mathematical proofs. Rigorous proofs are provided only for EW (Section~\ref{sec:conjectures}, Appendix~\ref{app:EW}) and partially for KPZ skewness (Appendix~\ref{app:KPZ}).

\subsection{Experimental Setup}

I summarize ML experiments providing empirical motivation~\cite{Bentley2026}:
\begin{itemize}
\item \textbf{Training classes}: EW, KPZ (50 samples each, $L=128$, $T=200$)
\item \textbf{Test classes}: MBE, conserved KPZ (VLDS), quenched-disorder KPZ
\item \textbf{Observable map}: $\Phi \in \bbR^{16}$ including gradients, exponents, spectral, morphological, temporal, correlation features
\item \textbf{Detector}: Isolation Forest (unsupervised anomaly detection)
\end{itemize}

\subsection{Detection Performance}

\begin{table}[h]
\centering
\caption{Anomaly detection performance across system sizes}
\label{tab:detection}
\begin{tabular}{lcccc}
\toprule
System Size & FPR & MBE & VLDS & Quenched KPZ \\
\midrule
$L=128$ (train) & 12.5\% & 100\% & 100\% & 100\% \\
$L=256$ & 12.5\% & 100\% & 100\% & 100\% \\
$L=512$ & 2.5\% & 100\% & 100\% & 100\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Interpretation:} 100\% detection $\Rightarrow$ $\supp(\mu^{\text{unknown}}_{L,T}) \cap \supp(\mu^{\EW+\KPZ}_{L,T}) = \emptyset$; FPR decrease $\Rightarrow$ $\delta(L,T)$ shrinks with $L$.

\textbf{Method Comparison:} Systematic comparison of anomaly detection methods reveals Isolation Forest (3\% FPR) outperforms Local Outlier Factor (4\%) and One-Class SVM (34\%). The SVM's poor performance stems from its assumption of convex decision boundaries, providing geometric evidence that universality class supports have non-convex structure in observable space.

\subsection{Similar-Exponent Validation: Ballistic Deposition}

A critical test of Conjecture~\ref{conj:separation} is whether separation persists when scaling exponents are \emph{identical}. Ballistic deposition (BD) has $\alpha \approx 0.5$---the same as both EW and KPZ---but exhibits fundamentally different local morphology due to discrete sticking dynamics.

\textbf{Results:}
\begin{itemize}
\item Detection rate: 100\% (50/50 BD surfaces classified as anomalous)
\item Cohen's $d$ separation from training data:
\begin{itemize}
\item Gradient features: $12{,}591\sigma$
\item Morphological features: $3{,}186\sigma$
\item Temporal features: $2{,}047\sigma$
\item Spectral features: $189\sigma$
\item Scaling exponents ($\alpha$, $\beta$): $0.43\sigma$ (indistinguishable)
\end{itemize}
\end{itemize}

\textbf{Theoretical Interpretation:} This demonstrates that Conjecture~\ref{conj:separation} holds even when $\alpha$ and $\beta$ are identical across classes. The separation is \emph{not} driven by scaling exponents but by morphological signatures of the underlying dynamics (discrete sticking vs. continuous diffusion). This validates the framework's central claim: universality class membership is encoded in observable-space geometry beyond what asymptotic exponents capture.

\subsection{Feature Ablation}

\begin{table}[h]
\centering
\caption{Detection rate by feature group}
\label{tab:ablation}
\begin{tabular}{lc}
\toprule
Feature Group & Detection Rate \\
\midrule
Gradient (1 feature) & \textbf{100\%} \\
Temporal (3 features) & \textbf{100\%} \\
Morphological (3 features) & 95.8\% \\
Correlation (3 features) & 83.3\% \\
Scaling $\alpha, \beta$ (2 features) & 79.2\% \\
Spectral (4 features) & 4.2\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Theoretical Puzzle:} Why do gradient statistics (100\%) outperform scaling exponents (79\%)?

\subsection{Resolution: Local vs. Global Observables}
\label{sec:gradient}

The conventional view: scaling exponents $(\alpha, \beta)$ are the ``universal'' quantities defining universality classes; gradient variance $\text{Var}(\nabla h) \sim L^{2\alpha-2}$ is a derived quantity.

\textbf{Resolution:} Consider the information-theoretic content of each observable:

\textbf{Scaling exponents:}
\begin{itemize}
\item Require fitting power laws over scale ranges
\item Subject to finite-size corrections: $\alpha_{\text{eff}}(L) = \alpha + a_1 L^{-\omega} + \cdots$
\item At $L=128$: $\alpha \approx 0.24$ vs theoretical $\alpha = 0.5$ (54\% error)
\end{itemize}

\textbf{Gradient statistics:}
\begin{itemize}
\item Direct measurement: compute $|\nabla h|$ at each point, average
\item CLT: fluctuations $\sim L^{-1/2}$ for local observables
\end{itemize}

\textbf{Formalization:} Let $\hat{\alpha}(h)$ be the exponent estimator and $\hat{\sigma}^2(h) = \text{Var}(\nabla h)$. The statistical properties differ:
\begin{align}
\text{Var}(\hat{\alpha}) &\sim O(1) \quad \text{(fitting uncertainty)} \\
\text{Var}(\hat{\sigma}^2) &\sim O(L^{-1}) \quad \text{(spatial averaging)}
\end{align}

\textbf{Broader Implication:} Local observables with CLT variance reduction may outperform theoretically-canonical global observables requiring fitting. The ballistic deposition test (Section~\ref{sec:empirical}) provides decisive evidence: BD has identical $\alpha \approx 0.5$ to EW/KPZ yet exhibits $12{,}591\sigma$ separation in gradient features versus $<1\sigma$ for exponents.

\subsection{Crossover Phenomena: Universality Distance}

For KPZ + biharmonic term:
\begin{equation}
\partial_t h = \nu\nabla^2 h + \frac{\lambda}{2}(\nabla h)^2 - \kappa\nabla^4 h + \eta
\end{equation}

\begin{definition}[Universality Distance]
Normalize anomaly scores:
\begin{equation}
D_{\text{ML}}(\kappa) = \frac{s(\kappa) - s(0)}{s(\infty) - s(0)}
\end{equation}
where $s(\kappa)$ is the Isolation Forest anomaly score.
\end{definition}

\textbf{Fit Results:} Bootstrap uncertainty quantification ($n=1000$) yields $D_{\text{ML}}(\kappa) = \kappa^\gamma/(\kappa^\gamma + \kappa_c^\gamma)$ with:
\begin{itemize}
\item Crossover scale: $\kappa_c = 0.876$ [95\% CI: 0.807, 0.938]
\item Sharpness: $\gamma = 1.537$ [95\% CI: 1.326, 1.775]
\item False positive rate: 5\% [95\% CI: 2\%, 9\%]
\item Fit quality: $R^2 = 0.964$
\end{itemize}

$D_{\text{ML}}$ provides $\sim 2\times$ better SNR than traditional exponent fitting in crossover regimes. The tight bootstrap confidence intervals demonstrate that the extracted parameters are robust to sample selection.

%==============================================================================
\section{Extensions}
\label{sec:extensions}
%==============================================================================

\subsection{Optimal Observable Selection}

\textbf{Information-Theoretic Formulation:} Maximize mutual information:
\begin{equation}
\Phi^* = \arg\max_\Phi I(\Phi(h); Y)
\end{equation}
subject to dimension constraint $\dim(\Phi) \leq d$.

\textbf{Practical Approaches:} VAEs, contrastive learning, information bottleneck.

\subsection{Extension to 2+1 Dimensions}

In 2+1D, KPZ exponents are known only numerically: $\alpha \approx 0.39$, $\beta \approx 0.24$~\cite{HalpinHealy2013}. The framework should generalize with appropriately chosen $\Phi$.

\subsection{Connections to Turbulence and Active Matter}

KPZ belongs to a broader class of systems with anomalous fluctuations. The Toner-Tu equations describing flocking share structural similarities~\cite{TonerTu1998}. Extension to these systems is a natural direction.

%==============================================================================
\section{Open Problems}
\label{sec:open}
%==============================================================================

\begin{problem}[Concentration Rate]
For EW, $\delta(L) \sim L^{-1/2}$. What is the rate for KPZ?
\end{problem}

\begin{problem}[Topology]
What is the correct topology for Conjecture~\ref{conj:geometric}?
\end{problem}

\begin{problem}[Canonical Observables]
Is there a principled selection of $\Phi$?
\end{problem}

\begin{problem}[Rigorous RG Connection]
Can Conjecture~\ref{conj:RG} be proved for exactly solvable cases?
\end{problem}

\begin{problem}[Geometric Universality as Definition]
Can Conjecture~\ref{conj:geometric} serve as a \emph{definition} of universality class membership?
\end{problem}

%==============================================================================
\section{Conclusion}
\label{sec:conclusion}
%==============================================================================

I have developed a measure-theoretic framework characterizing universality classes in stochastic surface growth as geometric objects in observable space. The central insight: universality classes correspond to distinct, concentrating supports of induced measures under observable maps.

\textbf{Key Contributions:}
\begin{enumerate}
\item Formal framework: growth processes, observable maps, induced measures with Wasserstein/MMD metrics
\item Central conjectures with partial proofs (EW rigorous, KPZ skewness bounds)
\item RG connection via conjectured correspondence $\mu^\Phi_\infty = \Phi_* \bbP^*$
\item Empirical validation with rigorous uncertainty quantification:
\begin{itemize}
\item 100\% detection of unknown classes with bootstrap-validated crossover parameters ($\kappa_c = 0.876$ [0.807, 0.938])
\item Method comparison: Isolation Forest (3\% FPR) optimal vs LOF (4\%) and OC-SVM (34\%)
\item Similar-exponent test: BD detection ($12{,}591\sigma$ gradient separation) despite $\alpha \approx 0.5$ matching training classes
\item Gradient features (100\%) outperform scaling exponents (79\%)
\end{itemize}
\end{enumerate}

This framework provides an \textbf{operational characterization of universality} accessible from finite-size data where traditional exponent fitting fails. The finding that local gradient statistics outperform traditional scaling exponents challenges conventional approaches and suggests: \emph{direct measurement of local fluctuations may encode universality more robustly than global fitting procedures at finite size}.

\begin{acknowledgments}
[To be added]
\end{acknowledgments}

%==============================================================================
\appendix
%==============================================================================

\section{Proof of EW Concentration}
\label{app:EW}

\textbf{Claim:} For Edwards-Wilkinson dynamics with $\Phi = \text{Var}(\nabla h)$, the effective diameter satisfies $\delta(L) \sim L^{-1/2}$.

\textbf{Proof:}
\begin{enumerate}
\item EW in 1+1D with periodic boundaries has stationary solution with height Fourier modes $\hat{h}_k = \int_0^L h(x) e^{-ikx} dx$, $k = 2\pi n/L$.

\item The stationary covariance: $\langle |\hat{h}_k|^2 \rangle = D/(\nu k^2)$ for $k \neq 0$.

\item Gradient variance: $\text{Var}(\nabla h) = (1/L) \sum_{k\neq 0} k^2 \langle |\hat{h}_k|^2 \rangle = (1/L) \sum_{k\neq 0} D/\nu \to D/\nu$.

\item For a single realization, the sample variance $\hat{\sigma}^2 = (1/L) \sum_x (\nabla h)^2 - [(1/L) \sum_x \nabla h]^2$ fluctuates around the mean with variance $O(1/L)$ by spatial CLT.

\item Therefore the distribution of $\hat{\sigma}^2$ concentrates: variance shrinks as $L^{-1}$, implying $\delta(L) \sim L^{-1/2}$. \hfill $\square$
\end{enumerate}

\section{Tracy-Widom Bounds for KPZ Separation}
\label{app:KPZ}

\textbf{Claim:} KPZ and EW are asymptotically separated by skewness observable.

\textbf{Argument:}
\begin{enumerate}
\item EW height fluctuations are Gaussian: skewness $\gamma_1 = 0$.
\item KPZ converges to Tracy-Widom with $\gamma_1 \approx 0.29$ (GOE) or $0.22$ (GUE)~\cite{Johansson2000,TracyWidom1994}.
\item For $\Phi = \hat{\gamma}_1$: $\mu^{\EW}_{L\to\infty}$ concentrated at $\gamma_1 = 0$; $\mu^{\KPZ}_{L\to\infty}$ at $\gamma_1 \approx 0.29$.
\item Separation distance $\sim 0.29 > 0$. \hfill $\square$
\end{enumerate}

\section{Summary of Rigor Status}
\label{app:rigor}

\begin{table}[h]
\centering
\caption{Classification of results by rigor level}
\begin{tabular}{lll}
\toprule
Result & Status & Justification \\
\midrule
EW Gaussian statistics & Rigorous & Exactly solvable \\
EW concentration $\delta(L)\sim L^{-1/2}$ & Rigorous & CLT (App.~\ref{app:EW}) \\
KPZ Tracy-Widom limit & Rigorous & Refs.~\cite{Sasamoto2010,Corwin2012} \\
KPZ skewness separation & Rigorous & App.~\ref{app:KPZ} \\
KPZ finite-size rate & Partial & Ferrari-Spohn~\cite{FerrariSpohn2006} \\
Conjectures 1--4 & Conjecture & Open \\
RG Correspondence & Heuristic & Plausible \\
ML detection rates & Empirical & Supporting evidence \\
\bottomrule
\end{tabular}
\end{table}

%==============================================================================
\bibliographystyle{apsrev4-2}
\bibliography{references}
%==============================================================================

\end{document}
