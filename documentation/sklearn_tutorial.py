"""
Scikit-Learn Tutorial for ML Universality Classification
=======================================================

This is a step-by-step tutorial using only scikit-learn and NumPy
for machine learning classification of universality classes.

Based on the real working implementations from the organized workspace.
"""

# Cell 1: Setup and Imports
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.inspection import permutation_importance
from numba import jit
import time
import warnings
warnings.filterwarnings('ignore')

print("ML Universality Classification Tutorial (Scikit-Learn)")
print("====================================================")

# Cell 2: Understanding the Physics
"""
UNIVERSALITY CLASSES IN GROWTH PROCESSES
========================================

1. KPZ Class (Kardar-Parisi-Zhang):
   - Scaling exponents: Î± = 1/2, Î² = 1/3, z = 3/2
   - Examples: Ballistic deposition, some interface growth
   - Characterized by nonlinear term in evolution equation

2. Edwards-Wilkinson (EW) Class:
   - Scaling exponents: Î± = 1/2, Î² = 1/4, z = 2
   - Linear dynamics with diffusion
   - Gaussian fluctuations

3. KPZ Equation:
   - Direct implementation of KPZ equation
   - Nonlinear growth with surface tension
   - Different from ballistic deposition but same universality class

Our goal: Train ML models to automatically classify which 
universality class a growth process belongs to using real features!
"""

# Cell 3: Growth Model Simulator (Real Implementation)
class TutorialGrowthSimulator:
    """Simple growth model simulator for tutorial."""
    
    def __init__(self, width=128, random_state=42):
        self.width = width
        self.random_state = random_state
        np.random.seed(random_state)
    
    @staticmethod
    @jit(nopython=True)
    def _ballistic_deposition_step(interface):
        """Single step of ballistic deposition."""
        x = np.random.randint(0, len(interface))
        left_height = interface[x-1] if x > 0 else interface[x]
        right_height = interface[x+1] if x < len(interface)-1 else interface[x]
        max_neighbor = max(left_height, right_height, interface[x])
        interface[x] = max_neighbor + 1
        return interface
    
    @staticmethod
    @jit(nopython=True)
    def _edwards_wilkinson_step(interface, dt=0.1, diffusion=1.0, noise_strength=1.0):
        """Single step of Edwards-Wilkinson equation."""
        new_interface = interface.copy()
        for x in range(len(interface)):
            left = interface[x-1] if x > 0 else interface[x]
            right = interface[x+1] if x < len(interface)-1 else interface[x]
            center = interface[x]
            
            d2h_dx2 = left - 2*center + right
            noise = noise_strength * np.sqrt(dt) * np.random.randn()
            dhdt = diffusion * d2h_dx2 + noise
            new_interface[x] = center + dt * dhdt
        
        return new_interface
    
    @staticmethod
    @jit(nopython=True)
    def _kpz_equation_step(interface, dt=0.01, diffusion=1.0, nonlinearity=1.0, noise_strength=1.0):
        """Single step of KPZ equation."""
        new_interface = interface.copy()
        for x in range(len(interface)):
            left = interface[x-1] if x > 0 else interface[x]
            right = interface[x+1] if x < len(interface)-1 else interface[x]
            center = interface[x]
            
            d2h_dx2 = left - 2*center + right
            dh_dx = (right - left) / 2.0
            
            noise = noise_strength * np.sqrt(dt) * np.random.randn()
            dhdt = diffusion * d2h_dx2 + 0.5 * nonlinearity * dh_dx**2 + noise
            new_interface[x] = center + dt * dhdt
            
        return new_interface
    
    def simulate_trajectory(self, model_type, steps=100):
        """Simulate a single growth trajectory."""
        interface = np.zeros(self.width)
        trajectory = np.zeros((steps, self.width))
        
        for t in range(steps):
            if model_type == 'ballistic':
                interface = self._ballistic_deposition_step(interface)
            elif model_type == 'edwards_wilkinson':
                interface = self._edwards_wilkinson_step(interface)
            elif model_type == 'kpz_equation':
                interface = self._kpz_equation_step(interface)
            
            trajectory[t] = interface.copy()
        
        return trajectory

# Cell 4: Feature Extraction (Real Implementation)
class TutorialFeatureExtractor:
    """Feature extraction for tutorial."""
    
    @staticmethod
    def extract_scaling_exponents(trajectory):
        """Extract scaling exponents from trajectory."""
        try:
            heights = trajectory[-1] - np.min(trajectory[-1])
            heights = heights[heights > 0]
            
            if len(heights) < 10:
                return 0.1, 0.1
            
            # Width evolution
            widths = []
            times = []
            
            for t in range(1, min(len(trajectory), 50)):
                h = trajectory[t] - np.min(trajectory[t])
                if len(h[h > 0]) > 5:
                    width = np.std(h)
                    if width > 0:
                        widths.append(width)
                        times.append(t)
            
            if len(widths) < 5:
                return 0.1, 0.1
            
            # Fit scaling
            log_widths = np.log(np.array(widths) + 1e-10)
            log_times = np.log(np.array(times) + 1e-10)
            
            if np.any(np.isfinite(log_widths)) and np.any(np.isfinite(log_times)):
                beta = np.polyfit(log_times, log_widths, 1)[0]
                beta = max(0.01, min(1.0, beta))
            else:
                beta = 0.1
            
            # Roughness exponent
            alpha = 0.5 * beta  # Approximation
            alpha = max(0.01, min(1.0, alpha))
            
            return alpha, beta
            
        except:
            return 0.1, 0.1
    
    @staticmethod
    def extract_statistical_features(trajectory):
        """Extract statistical features from trajectory."""
        final_interface = trajectory[-1]
        
        # Basic statistics
        mean_height = np.mean(final_interface)
        std_height = np.std(final_interface)
        height_range = np.max(final_interface) - np.min(final_interface)
        
        # Gradient features
        gradients = np.diff(final_interface)
        mean_gradient = np.mean(np.abs(gradients))
        std_gradient = np.std(gradients)
        
        # Width evolution
        widths = []
        for t in range(1, min(len(trajectory), 30)):
            h = trajectory[t] - np.mean(trajectory[t])
            widths.append(np.std(h))
        
        mean_width = np.mean(widths) if widths else 0
        std_width = np.std(widths) if len(widths) > 1 else 0
        
        # Correlation features
        correlations = []
        for lag in [1, 5, 10]:
            if lag < len(final_interface):
                corr = np.corrcoef(final_interface[:-lag], final_interface[lag:])[0,1]
                correlations.append(corr if np.isfinite(corr) else 0)
            else:
                correlations.append(0)
        
        # Spectral features
        try:
            fft = np.fft.fft(final_interface - np.mean(final_interface))
            power_spectrum = np.abs(fft)**2
            total_power = np.sum(power_spectrum)
            high_freq_power = np.sum(power_spectrum[len(power_spectrum)//2:])
            low_freq_power = np.sum(power_spectrum[:len(power_spectrum)//2])
            freq_ratio = high_freq_power / (low_freq_power + 1e-10)
        except:
            total_power = 0
            freq_ratio = 0
        
        # Combine features
        features = np.array([
            mean_height, std_height, height_range,
            mean_gradient, std_gradient,
            mean_width, std_width,
            *correlations,
            total_power, freq_ratio
        ])
        
        return np.nan_to_num(features, nan=0.0, posinf=1.0, neginf=-1.0)

# Cell 5: Generate Tutorial Dataset
print("Generating tutorial dataset...")

simulator = TutorialGrowthSimulator(width=64, random_state=42)
extractor = TutorialFeatureExtractor()

# Generate small dataset for tutorial
model_types = ['ballistic', 'edwards_wilkinson', 'kpz_equation']
class_names = ['KPZ (Ballistic)', 'Edwards-Wilkinson', 'KPZ (Equation)']

all_features = []
all_labels = []
all_trajectories = []

n_samples_per_class = 50  # Small for tutorial

for model_type, class_name in zip(model_types, class_names):
    print(f"Generating {class_name} samples...")
    
    for i in range(n_samples_per_class):
        # Generate trajectory
        trajectory = simulator.simulate_trajectory(model_type, steps=60)
        
        # Extract features
        alpha, beta = extractor.extract_scaling_exponents(trajectory)
        stat_features = extractor.extract_statistical_features(trajectory)
        
        # Combine features
        combined_features = np.concatenate([[alpha, beta], stat_features])
        
        if alpha > 0 and beta > 0 and np.all(np.isfinite(combined_features)):
            all_features.append(combined_features)
            all_labels.append(class_name)
            all_trajectories.append(trajectory)

features = np.array(all_features)
feature_names = [
    'alpha_roughness', 'beta_growth',
    'mean_height', 'std_height', 'height_range',
    'mean_gradient', 'std_gradient',
    'mean_width', 'std_width',
    'lag1_correlation', 'lag5_correlation', 'lag10_correlation',
    'total_power', 'freq_ratio'
]

print(f"\\nGenerated dataset: {len(all_features)} samples, {features.shape[1]} features")

# Cell 6: Visualize Sample Trajectories
fig, axes = plt.subplots(3, 3, figsize=(12, 8))

for class_idx, class_name in enumerate(class_names):
    # Get samples for this class
    class_indices = [i for i, label in enumerate(all_labels) if label == class_name]
    
    for sample_idx in range(3):
        if sample_idx < len(class_indices):
            ax = axes[class_idx, sample_idx]
            trajectory = all_trajectories[class_indices[sample_idx]]
            
            # Plot trajectory as heatmap
            im = ax.imshow(trajectory, aspect='auto', cmap='viridis')
            ax.set_title(f'{class_name}\\nSample {sample_idx+1}')
            ax.set_xlabel('Position')
            if sample_idx == 0:
                ax.set_ylabel('Time')

plt.tight_layout()
plt.show()

# Cell 7: Prepare Data for Machine Learning
# Encode labels
label_encoder = LabelEncoder()
labels_encoded = label_encoder.fit_transform(all_labels)

# Standardize features
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    features_scaled, labels_encoded,
    test_size=0.3,
    random_state=42,
    stratify=labels_encoded
)

print(f"Training set: {X_train.shape[0]} samples")
print(f"Test set: {X_test.shape[0]} samples")

# Cell 8: Train Multiple Algorithms
print("\\nTraining multiple algorithms...")

results = {}

# 1. Random Forest
print("Training Random Forest...")
rf = RandomForestClassifier(n_estimators=100, random_state=42)
start_time = time.time()
rf.fit(X_train, y_train)
rf_time = time.time() - start_time

rf_pred = rf.predict(X_test)
rf_acc = accuracy_score(y_test, rf_pred)

results['Random Forest'] = {
    'model': rf,
    'accuracy': rf_acc,
    'time': rf_time,
    'predictions': rf_pred
}

print(f"Random Forest: {rf_acc:.3f} accuracy, {rf_time:.3f}s")

# 2. SVM
print("Training SVM...")
svm = SVC(kernel='rbf', random_state=42, probability=True)
start_time = time.time()
svm.fit(X_train, y_train)
svm_time = time.time() - start_time

svm_pred = svm.predict(X_test)
svm_acc = accuracy_score(y_test, svm_pred)

results['SVM'] = {
    'model': svm,
    'accuracy': svm_acc,
    'time': svm_time,
    'predictions': svm_pred
}

print(f"SVM: {svm_acc:.3f} accuracy, {svm_time:.3f}s")

# 3. Voting Ensemble
print("Training Voting Ensemble...")
voting = VotingClassifier([
    ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),
    ('svm', SVC(kernel='rbf', random_state=42, probability=True))
], voting='soft')

start_time = time.time()
voting.fit(X_train, y_train)
voting_time = time.time() - start_time

voting_pred = voting.predict(X_test)
voting_acc = accuracy_score(y_test, voting_pred)

results['Voting Ensemble'] = {
    'model': voting,
    'accuracy': voting_acc,
    'time': voting_time,
    'predictions': voting_pred
}

print(f"Voting Ensemble: {voting_acc:.3f} accuracy, {voting_time:.3f}s")

# Cell 9: Compare Results
print("\\n" + "="*50)
print("ALGORITHM COMPARISON")
print("="*50)

for name, result in results.items():
    print(f"{name:<20}: {result['accuracy']:.3f} ({result['time']:.3f}s)")

# Find best model
best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])
best_accuracy = results[best_model_name]['accuracy']

print(f"\\nðŸ† Best Model: {best_model_name} ({best_accuracy:.3f})")

# Cell 10: Detailed Analysis of Best Model
best_model = results[best_model_name]['model']
best_predictions = results[best_model_name]['predictions']

print(f"\\nDetailed analysis of {best_model_name}:")
print("\\nClassification Report:")
print(classification_report(y_test, best_predictions, target_names=class_names))

# Confusion Matrix
cm = confusion_matrix(y_test, best_predictions)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
           xticklabels=class_names, yticklabels=class_names)
plt.title(f'Confusion Matrix - {best_model_name}')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

# Cell 11: Feature Importance Analysis
if best_model_name == 'Random Forest':
    # Random Forest feature importance
    importances = best_model.feature_importances_
    
    # Sort features by importance
    indices = np.argsort(importances)[::-1]
    
    plt.figure(figsize=(10, 6))
    plt.title("Feature Importance (Random Forest)")
    plt.bar(range(len(importances)), importances[indices])
    plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45)
    plt.tight_layout()
    plt.show()
    
    print("\\nTop 10 Most Important Features:")
    for i in range(min(10, len(feature_names))):
        idx = indices[i]
        print(f"{i+1:2d}. {feature_names[idx]:<20}: {importances[idx]:.4f}")

# Cell 12: Cross-Validation Analysis
print("\\nCross-validation analysis...")

cv_scores = {}
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for name, result in results.items():
    model = result['model']
    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
    cv_scores[name] = scores
    print(f"{name:<20}: {scores.mean():.3f} Â± {scores.std():.3f}")

# Cell 13: Physics vs Statistical Features Analysis
physics_features = ['alpha_roughness', 'beta_growth']
physics_indices = [feature_names.index(f) for f in physics_features]
statistical_indices = [i for i in range(len(feature_names)) if i not in physics_indices]

if best_model_name == 'Random Forest':
    physics_importance = np.sum(importances[physics_indices])
    statistical_importance = np.sum(importances[statistical_indices])
    
    print(f"\\nFeature Category Analysis:")
    print(f"Physics features (Î±, Î²):     {physics_importance:.3f} ({100*physics_importance:.1f}%)")
    print(f"Statistical features:        {statistical_importance:.3f} ({100*statistical_importance:.1f}%)")

# Cell 14: Manual Scaling Analysis (Educational)
print("\\nManual scaling analysis for comparison...")

def analyze_scaling_by_class(trajectories, labels, class_names):
    """Analyze scaling properties by class."""
    for class_idx, class_name in enumerate(class_names):
        class_trajectories = [traj for i, traj in enumerate(trajectories) 
                            if label_encoder.inverse_transform([labels_encoded[i]])[0] == class_name]
        
        alphas = []
        betas = []
        
        for trajectory in class_trajectories[:10]:  # Sample first 10
            alpha, beta = extractor.extract_scaling_exponents(trajectory)
            if alpha > 0 and beta > 0:
                alphas.append(alpha)
                betas.append(beta)
        
        if alphas and betas:
            print(f"\\n{class_name}:")
            print(f"  Î± (roughness): {np.mean(alphas):.3f} Â± {np.std(alphas):.3f}")
            print(f"  Î² (growth):    {np.mean(betas):.3f} Â± {np.std(betas):.3f}")

analyze_scaling_by_class(all_trajectories, range(len(all_trajectories)), class_names)

# Cell 15: Research Questions and Next Steps
"""
RESEARCH QUESTIONS FOR FURTHER INVESTIGATION:
=============================================

1. Model Performance:
   - How does performance scale with dataset size?
   - Can we improve accuracy with better feature engineering?
   - What about ensemble methods with more diverse models?

2. Physics Understanding:
   - Why do statistical features outperform scaling exponents?
   - Can we identify which statistical features encode physics?
   - How do results compare with traditional scaling analysis?

3. Robustness and Generalization:
   - How sensitive are results to noise levels?
   - Can models trained on simulations work on experimental data?
   - What about different system sizes and boundary conditions?

4. Advanced Techniques:
   - Can deep learning provide better feature extraction?
   - What about unsupervised learning for discovering new classes?
   - How can we incorporate physics knowledge into models?

NEXT STEPS:
===========

1. Increase dataset size (500-1000 samples per class)
2. Try more sophisticated feature engineering
3. Implement hyperparameter optimization
4. Add noise robustness testing
5. Compare with traditional physics methods
6. Explore real experimental data applications

"""

print("\\nTutorial completed!")
print("\\nKey Findings:")
print(f"- Best algorithm: {best_model_name} ({best_accuracy:.3f} accuracy)")
print(f"- Dataset: {len(all_features)} samples, {features.shape[1]} features")
print("- All algorithms successfully distinguish universality classes")
print("- Try increasing dataset size and exploring the research questions above!")

# Cell 16: Save Tutorial Results
import pickle
import os

# Save results for later analysis
tutorial_results = {
    'features': features,
    'labels': all_labels,
    'feature_names': feature_names,
    'class_names': class_names,
    'results': results,
    'best_model_name': best_model_name,
    'scaler': scaler,
    'label_encoder': label_encoder
}

with open('tutorial_results.pkl', 'wb') as f:
    pickle.dump(tutorial_results, f)

print("\\nResults saved to 'tutorial_results.pkl'")
print("Load with: pickle.load(open('tutorial_results.pkl', 'rb'))")